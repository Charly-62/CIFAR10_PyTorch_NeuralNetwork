{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (Neural networks with PyTorch)\n",
    "\n",
    "In this problem, the goal is to utilize the PyTorch library to train a simple fully connected neural network to classify RGB color images from the CIFAR10 dataset. The dataset consist of 10 classes plane, car, bird, cat, deer, dog, frog, horse, ship, truck, where there are 5000 training and 1000 test images per class. Each image has dimensions $ 3 \\times 32 \\times 32 $. The dataset also includes labels for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR10 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "In this problem we only consider images of the classes cat, dog and ship and train/test on 3000/1000 images per class. Start by reducing the size of the training and test set accordingly. Then, initialize the train and test dataloaders. We use a batch size of $ B = 4 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = [3, 5, 8]\n",
    "\n",
    "def build_subset(dataset, target_classes, samples):\n",
    "    targets = np.array(dataset.targets)\n",
    "    indices = []\n",
    "    for i in target_classes:\n",
    "        indices.extend(np.where(targets == i)[0][:samples])\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "trainset = build_subset(trainset, class_indices, 3000)\n",
    "testset = build_subset(testset, class_indices, 1000)\n",
    "\n",
    "class_mapping = {original_class: new_index for new_index, original_class in enumerate(class_indices)}\n",
    "trainset.dataset.targets = [class_mapping[label] if label in class_indices else label for label in trainset.dataset.targets]\n",
    "testset.dataset.targets = [class_mapping[label] if label in class_indices else label for label in testset.dataset.targets]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "Inheriting from nn.Module, implement a simple fully connected neural network with a single hidden layer of dimension 512, ReLU activations (not for the output layer) and an output dimension equal to the number of classes. Note that for the first linear layer to process a batch of input images the batch needs to be flattened across the color channels and spatial dimensions to a size of $ B \\times 3072 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 32 * 32, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "Define a SGD optimizer with learning rate `0.001` and momentum `0.9` from torch.optim. With momentum the optimizer adds a weighted running average of the gradients per parameter to the current gradient in each step improving both stability and speed of convergence. Use the torch.nn.CrossEntropyLoss() loss and train the network for 10 epochs. Report the train and test accuracy after every epoch and save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Error: \n",
      " Accuracy: 63.89%, Avg loss: 0.724714 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.77%, Avg loss: 0.688844 \n",
      "\n",
      "Epoch 2\n",
      "Train Error: \n",
      " Accuracy: 68.60%, Avg loss: 0.650457 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.07%, Avg loss: 0.695679 \n",
      "\n",
      "Epoch 3\n",
      "Train Error: \n",
      " Accuracy: 70.38%, Avg loss: 0.607824 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.47%, Avg loss: 0.713574 \n",
      "\n",
      "Epoch 4\n",
      "Train Error: \n",
      " Accuracy: 73.38%, Avg loss: 0.568773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.23%, Avg loss: 0.682848 \n",
      "\n",
      "Epoch 5\n",
      "Train Error: \n",
      " Accuracy: 74.09%, Avg loss: 0.542213 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.40%, Avg loss: 0.704923 \n",
      "\n",
      "Epoch 6\n",
      "Train Error: \n",
      " Accuracy: 76.64%, Avg loss: 0.506158 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.67%, Avg loss: 0.726577 \n",
      "\n",
      "Epoch 7\n",
      "Train Error: \n",
      " Accuracy: 78.61%, Avg loss: 0.483516 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.40%, Avg loss: 0.759970 \n",
      "\n",
      "Epoch 8\n",
      "Train Error: \n",
      " Accuracy: 80.32%, Avg loss: 0.448310 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.47%, Avg loss: 0.725484 \n",
      "\n",
      "Epoch 9\n",
      "Train Error: \n",
      " Accuracy: 81.94%, Avg loss: 0.421566 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.30%, Avg loss: 0.784895 \n",
      "\n",
      "Epoch 10\n",
      "Train Error: \n",
      " Accuracy: 83.04%, Avg loss: 0.395804 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.07%, Avg loss: 0.821584 \n",
      "\n",
      "Done training\n",
      "Best model saved with accuracy: 69.47%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    avg_trainloss = train_loss / len(dataloader)\n",
    "    train_accuracy = correct / size\n",
    "    print(f\"Train Error: \\n Accuracy: {(100*train_accuracy):>0.2f}%, Avg loss: {avg_trainloss:>5f} \\n\")\n",
    "    \n",
    "    return avg_trainloss, train_accuracy\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    avg_testloss = test_loss / len(dataloader)\n",
    "    test_accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):>0.2f}%, Avg loss: {avg_testloss:>5f} \\n\")\n",
    "    \n",
    "    return avg_testloss, test_accuracy\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    train_loss, train_accuracy = train(trainloader, model, loss_fn, optimizer)\n",
    "    test_loss, test_accuracy = test(testloader, model, loss_fn)\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "print(\"Done training\")\n",
    "\n",
    "if best_model:\n",
    "    torch.save(best_model, \"best_model.pth\")\n",
    "    print(f\"Best model saved with accuracy: {best_accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "Load the best model and report the overall test accuracy and the test accuracy per class. On which class does our classifier perform best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 68.07%\n",
      "Accuracy for class cat: 54.90%\n",
      "Accuracy for class dog: 62.40%\n",
      "Accuracy for class ship: 86.90%\n"
     ]
    }
   ],
   "source": [
    "bestmodel = NeuralNetwork().to(device)\n",
    "bestmodel.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n",
    "\n",
    "bestmodel.eval()\n",
    "\n",
    "total_correct, total_samples = 0, 0\n",
    "\n",
    "class_correct = [0 for _ in range(len(class_indices))]\n",
    "class_total = [0 for _ in range(len(class_indices))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = bestmodel(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "overall_accuracy = total_correct / total_samples\n",
    "class_accuracies = [class_correct[i] / class_total[i] for i in range(len(class_indices))]\n",
    "\n",
    "print(f\"Overall accuracy: {overall_accuracy*100:.2f}%\")\n",
    "\n",
    "classes = [\"cat\", \"dog\", \"ship\"]\n",
    "\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Accuracy for class {classes[i]}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier performs best on the \"ship\" class with an accuracy of 86.90%, compared to around 60% for \"cat\" and \"dog.\" This suggests that the model finds it easier to distinguish ships, likely because cats and dogs share similar animal features, making them harder to differentiate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
